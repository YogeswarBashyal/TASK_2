{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7d6df33-f335-4191-897f-a940d35ad64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5030a0b4-af28-4699-9124-90faf12ce4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load datasets\n",
    "train_df = pd.read_csv(\"../yoges/Desktop/Deerwalk/dataset/TASK_2/TASK_2/train_set.csv\")\n",
    "test_df = pd.read_csv(\"../yoges/Desktop/Deerwalk/dataset/TASK_2/TASK_2/test_set.csv\")\n",
    "blind_df = pd.read_csv(\"../yoges/Desktop/Deerwalk/dataset/TASK_2/TASK_2/blinded_test_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c9efb2d-a1ec-46dd-bb70-f1df3e3f82a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (315, 3240)\n",
      "Test shape: (100, 3240)\n",
      "Blind shape: (36, 3239)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"Blind shape: {blind_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec70dafb-2331-4602-be12-9ea28e8c9fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Missing and infinite value summary function\n",
    "def summary_missing_inf(df, df_name):\n",
    "    print(f\"--- {df_name} ---\")\n",
    "    total_elements = df.size\n",
    "    missing = df.isnull().sum().sum()\n",
    "    infinite = np.isinf(df.select_dtypes(include=[np.number])).sum().sum()\n",
    "    print(f\"Missing values: {missing} ({missing/total_elements*100:.2f}%)\")\n",
    "    print(f\"Infinite values: {infinite}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e88e692-3c6a-43c0-a761-16b9376cf82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Train ---\n",
      "Missing values: 2668 (0.26%)\n",
      "Infinite values: 4\n",
      "\n",
      "--- Test ---\n",
      "Missing values: 1127 (0.35%)\n",
      "Infinite values: 0\n",
      "\n",
      "--- Blind ---\n",
      "Missing values: 276 (0.24%)\n",
      "Infinite values: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary_missing_inf(train_df, \"Train\")\n",
    "summary_missing_inf(test_df, \"Test\")\n",
    "summary_missing_inf(blind_df, \"Blind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff30473d-e6a9-4bcc-89df-646d901c62b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Separate features and target\n",
    "X_train = train_df.drop(columns=['ID', 'CLASS'])\n",
    "y_train = train_df['CLASS']\n",
    "X_test = test_df.drop(columns=['ID', 'CLASS'])\n",
    "y_test = test_df['CLASS']\n",
    "X_blind = blind_df.drop(columns=['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8bfca43-f279-46d1-b4e1-8049d7e6d3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Replace infinite values with NaN for all datasets to handle later\n",
    "X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_blind.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52608c29-e01e-4334-bde9-5b5175aadf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. KNN imputation for selected features that need it (customize list per your data)\n",
    "knn_features = ['Feature_1712', 'Feature_1713', 'Feature_1714', 'Feature_1715',\n",
    "                'Feature_1716', 'Feature_1717', 'Feature_1718', 'Feature_1719',\n",
    "                'Feature_1725', 'Feature_1729']\n",
    "scaler_knn = StandardScaler()\n",
    "knn_imputer = KNNImputer(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa5fdec9-d839-422f-99dc-ce301454cebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute train KNN features\n",
    "subset_train = X_train[knn_features]\n",
    "subset_train_scaled = scaler_knn.fit_transform(subset_train)\n",
    "subset_train_imputed = scaler_knn.inverse_transform(knn_imputer.fit_transform(subset_train_scaled))\n",
    "X_train.loc[:, knn_features] = subset_train_imputed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c68be2cf-158a-4d98-91db-ddeef47c21d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute test and blind KNN features\n",
    "for X in [X_test, X_blind]:\n",
    "    subset = X[knn_features]\n",
    "    subset_scaled = scaler_knn.transform(subset)\n",
    "    subset_imputed = scaler_knn.inverse_transform(knn_imputer.transform(subset_scaled))\n",
    "    X.loc[:, knn_features] = subset_imputed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea232dc3-1b3f-4e0d-a062-0569b01480d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Mean imputation for remaining features\n",
    "num_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "X_train.loc[:, num_cols] = mean_imputer.fit_transform(X_train[num_cols])\n",
    "X_test.loc[:, num_cols] = mean_imputer.transform(X_test[num_cols])\n",
    "X_blind.loc[:, num_cols] = mean_imputer.transform(X_blind[num_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72a478ae-3756-4546-910a-59532e2de0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing after imputation - Train: 0, Test: 0, Blind: 0\n"
     ]
    }
   ],
   "source": [
    "# Verify no missing\n",
    "print(f\"Missing after imputation - Train: {X_train.isnull().sum().sum()}, Test: {X_test.isnull().sum().sum()}, Blind: {X_blind.isnull().sum().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "752a3bdc-24ea-4e2a-8f34-b3a8363b1578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_blind_scaled = scaler.transform(X_blind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "564752fe-7adc-4728-8f34-24b56ead6ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Feature selection - Select top 100 features by mutual information\n",
    "selector = SelectKBest(mutual_info_classif, k=100)\n",
    "X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
    "X_test_selected = selector.transform(X_test_scaled)\n",
    "X_blind_selected = selector.transform(X_blind_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c747b859-e437-45f4-a744-6992600f5827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Handle class imbalance with SMOTE on train data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train_selected, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c6ea01e7-ea7e-4e42-9ec7-45fb4ffaa5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Models and hyperparameter grids\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'XGBoost': XGBClassifier(eval_metric='logloss', use_label_encoder=False, random_state=42)\n",
    "}\n",
    "param_grids = {\n",
    "    'Logistic Regression': {'C': [0.001, 0.01, 0.1, 1, 10], 'penalty': ['l2']},\n",
    "    'Random Forest': {'n_estimators': [50,100,200], 'max_depth': [3,5,10], 'min_samples_split':[2,5]},\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [50,100,200],\n",
    "        'max_depth': [3,5,7],\n",
    "        'learning_rate': [0.01,0.1,0.3],\n",
    "        'gamma': [0,0.1],\n",
    "        'reg_lambda': [1,10]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "980d1678-b3b5-4519-ab99-9ff65103d61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "171322f7-9a0a-4546-8453-145e0bd31bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Function for specificity\n",
    "def specificity_score(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return tn / (tn + fp) if (tn + fp) > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "61ce149e-5644-4adf-9d80-da11905bf9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Metric evaluation function\n",
    "def evaluate_metrics(y_true, y_pred, y_proba):\n",
    "    return {\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'AUROC': roc_auc_score(y_true, y_proba),\n",
    "        'Sensitivity': recall_score(y_true, y_pred),\n",
    "        'Specificity': specificity_score(y_true, y_pred),\n",
    "        'F1-Score': f1_score(y_true, y_pred)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "48590d54-eb7f-4109-93ca-8a033f137a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and tuning Logistic Regression...\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best params for Logistic Regression: {'C': 0.01, 'penalty': 'l2'}\n",
      "\n",
      "Training and tuning Random Forest...\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best params for Random Forest: {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "\n",
      "Training and tuning XGBoost...\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yoges\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [16:56:48] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for XGBoost: {'gamma': 0, 'learning_rate': 0.3, 'max_depth': 5, 'n_estimators': 200, 'reg_lambda': 1}\n"
     ]
    }
   ],
   "source": [
    "# 13. Train / tune / evaluate models and save predictions\n",
    "results = []\n",
    "output_dir = 'predictions'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining and tuning {name}...\")\n",
    "    grid = GridSearchCV(model, param_grids[name], cv=cv, scoring='f1', n_jobs=-1, verbose=1)\n",
    "    grid.fit(X_train_res, y_train_res)\n",
    "    best_model = grid.best_estimator_\n",
    "    print(f\"Best params for {name}: {grid.best_params_}\")\n",
    "    \n",
    "    # Evaluate on train (resampled)\n",
    "    y_train_pred = best_model.predict(X_train_res)\n",
    "    y_train_proba = best_model.predict_proba(X_train_res)[:, 1]\n",
    "    train_metrics = evaluate_metrics(y_train_res, y_train_pred, y_train_proba)\n",
    "    train_metrics.update({'Model': name, 'Dataset': 'Train (Resampled)', 'Best_Params': grid.best_params_})\n",
    "    \n",
    "    # Evaluate on test\n",
    "    y_test_pred = best_model.predict(X_test_selected)\n",
    "    y_test_proba = best_model.predict_proba(X_test_selected)[:, 1]\n",
    "    test_metrics = evaluate_metrics(y_test, y_test_pred, y_test_proba)\n",
    "    test_metrics.update({'Model': name, 'Dataset': 'Test', 'Best_Params': grid.best_params_})\n",
    "    \n",
    "    results.append(train_metrics)\n",
    "    results.append(test_metrics)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0f4aea95-b6e6-4c89-b9c4-693aeaad217d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions: predictions\\xgboost_train_predictions.csv\n",
      "Saved predictions: predictions\\xgboost_test_predictions.csv\n",
      "Saved predictions: predictions\\xgboost_blinded_test_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Save predictions for train, test, and blinded test datasets\n",
    "for dataset_label, X_data_sel, orig_df in [('train', X_train_selected, train_df),\n",
    "                                               ('test', X_test_selected, test_df),\n",
    "                                               ('blinded_test', X_blind_selected, blind_df)]:\n",
    "        proba = best_model.predict_proba(X_data_sel)\n",
    "        proba_df = pd.DataFrame(proba, columns=[f'Class_{i}_Prob' for i in range(proba.shape[1])])\n",
    "        proba_df['ID'] = orig_df['ID'].reset_index(drop=True)\n",
    "        proba_df = proba_df[['ID'] + [f'Class_{i}_Prob' for i in range(proba.shape[1])]]\n",
    "        file_path = os.path.join(output_dir, f\"{name.lower().replace(' ', '_')}_{dataset_label}_predictions.csv\")\n",
    "        proba_df.to_csv(file_path, index=False)\n",
    "        print(f\"Saved predictions: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d044317b-58df-489f-947a-ce0077d9be1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics evaluation summary saved to: model_evaluation_metrics.csv\n",
      "   Accuracy     AUROC  Sensitivity  Specificity  F1-Score  \\\n",
      "0  0.675393  0.741071     0.654450     0.696335  0.668449   \n",
      "1  0.620000  0.689245     0.595238     0.637931  0.568182   \n",
      "2  0.997382  1.000000     0.994764     1.000000  0.997375   \n",
      "3  0.590000  0.659278     0.404762     0.724138  0.453333   \n",
      "4  1.000000  1.000000     1.000000     1.000000  1.000000   \n",
      "5  0.590000  0.595238     0.357143     0.758621  0.422535   \n",
      "\n",
      "                 Model            Dataset  \\\n",
      "0  Logistic Regression  Train (Resampled)   \n",
      "1  Logistic Regression               Test   \n",
      "2        Random Forest  Train (Resampled)   \n",
      "3        Random Forest               Test   \n",
      "4              XGBoost  Train (Resampled)   \n",
      "5              XGBoost               Test   \n",
      "\n",
      "                                         Best_Params  \n",
      "0                       {'C': 0.01, 'penalty': 'l2'}  \n",
      "1                       {'C': 0.01, 'penalty': 'l2'}  \n",
      "2  {'max_depth': 10, 'min_samples_split': 5, 'n_e...  \n",
      "3  {'max_depth': 10, 'min_samples_split': 5, 'n_e...  \n",
      "4  {'gamma': 0, 'learning_rate': 0.3, 'max_depth'...  \n",
      "5  {'gamma': 0, 'learning_rate': 0.3, 'max_depth'...  \n"
     ]
    }
   ],
   "source": [
    "# 14. Save metrics results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_csv_path = 'model_evaluation_metrics.csv'\n",
    "results_df.to_csv(results_csv_path, index=False)\n",
    "print(\"\\nMetrics evaluation summary saved to:\", results_csv_path)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da62365-64fd-43da-8e5c-261bac2eeff7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
